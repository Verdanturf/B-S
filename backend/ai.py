# backend/ai.py
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from transformers import pipeline
import dateparser 
from dateparser.search import search_dates

# å…¨å±€å˜é‡
classifier_scene = None
classifier_object = None
extractor_ner = None

def load_models():
    global classifier_scene, classifier_object, extractor_ner
    print("ğŸ¤– æ­£åœ¨åŠ è½½ AI æ··åˆå¼•æ“...")
    try:
        # 1. è§†è§‰æ¨¡å‹
        if classifier_scene is None:
            print("   - [1/3] Loading Scene Model...")
            classifier_scene = pipeline("image-classification", model="google/vit-base-patch16-224")
        if classifier_object is None:
            print("   - [2/3] Loading Object Model...")
            classifier_object = pipeline("object-detection", model="facebook/detr-resnet-50")
        
        # 2. æ–‡æœ¬æ¨¡å‹
        if extractor_ner is None:
            print("   - [3/3] Loading Text NER Model...")
            extractor_ner = pipeline("token-classification", model="uer/roberta-base-finetuned-cluener2020-chinese")
            
        print("âœ… AI å¼•æ“åŠ è½½å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

def analyze_image(image_path):
    """è§†è§‰åˆ†æ (ä¿æŒä¸å˜)"""
    global classifier_scene, classifier_object
    if classifier_scene is None: load_models()
    if classifier_scene is None: return None

    final_tags = set()
    try:
        res_scene = classifier_scene(image_path)
        if res_scene:
            final_tags.add(res_scene[0]['label'].split(',')[0].lower())
        res_objects = classifier_object(image_path)
        for obj in res_objects:
            if obj['score'] > 0.9:
                final_tags.add(obj['label'].lower())
        return ", ".join(list(final_tags))
    except Exception as e:
        return None

def analyze_text(text):
    """
    å‡çº§ç‰ˆæ–‡æœ¬åˆ†æï¼š
    1. ä½¿ç”¨ dateparser å¼ºåŠ›è§£ææ—¶é—´
    2. ä½¿ç”¨ NER æå–åœ°ç‚¹ (æ”¾å®½é™åˆ¶)
    """
    global extractor_ner
    if extractor_ner is None: load_models()
    
    extracted = {"location": None, "date": None}
    if not text: return extracted
    
    # --- 1. å¼ºåŠ›æ—¶é—´è§£æ (ä¼˜å…ˆä½¿ç”¨ dateparser) ---
    try:
        # search_dates ä¼šè‡ªåŠ¨ä»å¥å­é‡Œæ‰¾æ—¶é—´ï¼Œè¿”å› [(å­—ç¬¦ä¸², datetimeå¯¹è±¡), ...]
        # settings={'PREFER_DATES_FROM': 'future'} ä¹Ÿå¯ä»¥è®¾ç½®ï¼Œè¿™é‡Œç”¨é»˜è®¤
        dates = search_dates(text, languages=['zh'])
        if dates:
            # å–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ—¶é—´
            print(f"â° è§£æåˆ°æ—¶é—´: {dates[0]}")
            extracted['date'] = dates[0][1] # ç›´æ¥æ‹¿åˆ° datetime å¯¹è±¡
    except Exception as e:
        print(f"æ—¶é—´è§£æå¤±è´¥: {e}")

    # --- 2. åœ°ç‚¹æå– (NER) ---
    if extractor_ner:
        try:
            results = extractor_ner(text, aggregation_strategy="simple")
            loc_fragments = []
            
            for entity in results:
                # åªè¦æ˜¯ åœ°ç‚¹(LOC)ã€åœ°å€(address)ã€æœºæ„(ORG) éƒ½ç®—è¿›å»
                # å³ä½¿æ˜¯å•å­—ï¼ˆå¦‚â€œçœâ€ï¼‰ä¹Ÿä¸è¿‡æ»¤äº†ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
                if entity['entity_group'] in ['LOC', 'address', 'ORG']:
                    loc_fragments.append(entity['word'])
            
            # ç®€å•çš„æ‹¼æ¥ï¼Œä¸å»é‡ï¼ˆå› ä¸ºæœ‰æ—¶å€™â€œæµ™æ±Ÿâ€å’Œâ€œå¤§å­¦â€å¯èƒ½åˆ†å¼€è¯†åˆ«ï¼Œå»é‡ä¼šä¹±ï¼‰
            if loc_fragments:
                extracted['location'] = "".join(loc_fragments)
                
        except Exception as e:
            print(f"åœ°ç‚¹è§£æå¤±è´¥: {e}")
            
    return extracted